{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "omCLgUOB2_ZF"
   },
   "source": [
    "# \ud83c\udf1f XL LoRA Trainer (Lightning.ai) por Mighty Crimson\n",
    "\n",
    "Este notebook adapta el flujo de trabajo del proyecto original de [hollowstrawberry](https://colab.research.google.com/github/hollowstrawberry/kohya-colab/blob/main/Lora_Trainer_XL.ipynb) y el fork de [whitez](https://colab.research.google.com/github/gwhitez/Lora-Trainer-XL/blob/main/Fix_Lora_Trainer_XL.ipynb) usando [Kohya](https://github.com/kohya-ss/sd-scripts/tree/5a18a03ffcc2a21c6e884a25d041076911a79a2a) a para ejecutarse dentro de Lightning.ai.<br>\n",
    "Si se les hace compicado entender todo este notebook, comienzen primero haciendo loras en colab para familiarizarse con este notebook.<br>\n",
    "Recomendable solo usar la grafica L4, ya que es suficiente para todo.<br>\n",
    "Todo el trabajo se realiza en el directorio base `/teamspace/studios/this_studio`.\n",
    "\n",
    "> Basado en el trabajo de [Kohya-ss](https://github.com/kohya-ss/sd-scripts), [Linaqruf](https://github.com/Linaqruf/kohya-trainer) y los colaboradores de este fork.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vJ8clWTZEu-g"
   },
   "source": [
    "### \u2b55 Aviso\n",
    "Este cuaderno est\u00e1 pensado para investigaci\u00f3n y entrenamiento de modelos LoRA de forma responsable.\n",
    "Aseg\u00farate de respetar los t\u00e9rminos de servicio de Lightning.ai y de cualquier repositorio o dataset que utilices.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dPQlB4djNm3C"
   },
   "source": [
    "| Recurso | Enlace |\n",
    "| :--- | :--- |\n",
    "| C\u00f3digo base del entrenador | [gwhitez/LoRA_Easy_Training_scripts_Backend](https://github.com/gwhitez/LoRA_Easy_Training_scripts_Backend) |\n",
    "| Scripts originales | [kohya-ss/sd-scripts](https://github.com/kohya-ss/sd-scripts) |\n",
    "| Adaptaci\u00f3n Lightning | Este notebook |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h1 style='color: yellow;'>Dependencias Instaladas, Reinicia el Kernel</h1>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Dependencias, solo inicia una vez esta celda y reinicia el kernel\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "from IPython.display import clear_output\n",
    "import shutil\n",
    "!git clone https://github.com/MightyCrimsonX/LoRA_Easy_Training_scripts_Backend.git\n",
    "\n",
    "packages = [\n",
    "    \"accelerate==0.33.0\",\n",
    "    \"xformers\",\n",
    "    \"transformers==4.44.0\",\n",
    "    \"diffusers==0.25.0\",\n",
    "    \"torch\",\n",
    "    \"torchvision\",\n",
    "    \"ftfy==6.1.1\",\n",
    "    \"opencv-python==4.8.1.78\",\n",
    "    \"einops==0.7.0\",\n",
    "    \"pytorch-lightning==1.9.0\",\n",
    "    \"bitsandbytes==0.48.2\",\n",
    "    \"lion-pytorch==0.0.6\",\n",
    "    \"schedulefree==1.4\",\n",
    "    \"prodigy-plus-schedule-free==1.9.0\",\n",
    "    \"prodigyopt==1.1.2\",\n",
    "    \"tensorboard\",\n",
    "    \"safetensors==0.4.4\",\n",
    "    \"altair==4.2.2\",\n",
    "    \"easygui==0.98.3\",\n",
    "    \"toml==0.10.2\",\n",
    "    \"voluptuous==0.13.1\",\n",
    "    \"huggingface-hub==0.24.5\",\n",
    "    \"imagesize==1.4.1\",\n",
    "    \"numpy<=2.0\",\n",
    "    \"rich==13.7.0\",\n",
    "    \"sentencepiece==0.2.0\",\n",
    "    \"tarlette\",\n",
    "    \"uvicorn[standard]\",\n",
    "    \"requests\",\n",
    "    \"dadaptation\",\n",
    "    \"wandb\",\n",
    "    \"pyngrok\",\n",
    "    \"pycloudflared\",\n",
    "    \"scipy\",\n",
    "    \"came-pytorch\",\n",
    "    \"pytorch_optimizer==3.1.2\",\n",
    "    \"wheel\"\n",
    "]\n",
    "\n",
    "# Instalar cada paquete individualmente\n",
    "for package in packages:\n",
    "    !pip install {package}\n",
    "!sudo apt install aria2 -q\n",
    "%cd /teamspace/studios/this_studio/LoRA_Easy_Training_scripts_Backend\n",
    "!git clone https://github.com/kohya-ss/sd-scripts.git\n",
    "clear_output()\n",
    "display(HTML(f\"<h1 style='color: yellow;'>Dependencias Instaladas, Reinicia el Kernel</h1>\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Aqui creas la carpeta de tu lora a entrenar en \"project_name\"\n",
    "from pathlib import Path\n",
    "from IPython.display import display, HTML\n",
    "base_path = Path(\"/teamspace/studios/this_studio\")\n",
    "loras_path = base_path / \"lora_projects\"\n",
    "project_name = \" \"\n",
    "project_path = loras_path / project_name\n",
    "dataset_path = project_path / \"dataset\"\n",
    "\n",
    "loras_path.mkdir(parents=True, exist_ok=True)\n",
    "project_path.mkdir(exist_ok=True)\n",
    "dataset_path.mkdir(exist_ok=True)\n",
    "\n",
    "display(HTML(f\"<h1 style='color: yellow;'>Carpetas creadas, coloca tus imagenes y .txt en la carpeta dataset</h1>\"))\n",
    "display(HTML(f\"<h1 style='color: yellow;'>No te olvides de tambien configurar tus presets y poner tambien el mismo nombre del lora en la celda de abajo!</h1>\"))"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "#@title \ud83d\udd27 Configuraci\u00f3n r\u00e1pida del entrenamiento\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "training_model_options = [\n",
    "    \"Pony Diffusion V6 XL\",\n",
    "    \"Animagine XL V3\",\n",
    "    \"animagine_4.0_zero\",\n",
    "    \"Illustrious_0.1\",\n",
    "    \"Illustrious_2.0\",\n",
    "    \"NoobAI-XL0.75\",\n",
    "    \"NoobAI-XL0.5\",\n",
    "    \"Stable Diffusion XL 1.0 base\",\n",
    "    \"NoobAIXL0_75vpred\",\n",
    "    \"RouWei_v080vpred\",\n",
    "]\n",
    "training_model_default = \"Illustrious_2.0\"\n",
    "\n",
    "force_load_diffusers_options = [\n",
    "    (\"False\", False),\n",
    "    (\"True\", True),\n",
    "]\n",
    "\n",
    "lr_scheduler_options = [\n",
    "    \"constant\",\n",
    "    \"cosine\",\n",
    "    \"cosine_with_restarts\",\n",
    "    \"constant_with_warmup\",\n",
    "    \"rex\",\n",
    "]\n",
    "lr_scheduler_default = \"constant_with_warmup\"\n",
    "\n",
    "lora_type_options = [\"LoRA\", \"LoCon\"]\n",
    "\n",
    "precision_options = [\n",
    "    \"full fp16\",\n",
    "    \"full bf16\",\n",
    "    \"mixed fp16\",\n",
    "    \"mixed bf16\",\n",
    "]\n",
    "\n",
    "optimizer_options = [\n",
    "    \"AdamW8bit\",\n",
    "    \"Prodigy\",\n",
    "    \"DAdaptation\",\n",
    "    \"DadaptAdam\",\n",
    "    \"DadaptLion\",\n",
    "    \"AdamW\",\n",
    "    \"AdaFactor\",\n",
    "    \"Came\",\n",
    "]\n",
    "\n",
    "def _coalesce(value, options, fallback):\n",
    "    if value in options:\n",
    "        return value\n",
    "    return fallback\n",
    "\n",
    "def _format_scientific(value):\n",
    "    try:\n",
    "        return format(float(value), \".0e\")\n",
    "    except (TypeError, ValueError):\n",
    "        return str(value)\n",
    "\n",
    "project_name_widget = widgets.Text(\n",
    "    value=str(globals().get(\"project_name\", \"\")),\n",
    "    description=\"project_name\",\n",
    "    placeholder=\"Nombre del proyecto\",\n",
    ")\n",
    "\n",
    "training_model_widget = widgets.Dropdown(\n",
    "    options=training_model_options,\n",
    "    value=_coalesce(globals().get(\"training_model\", training_model_default), training_model_options, training_model_default),\n",
    "    description=\"training_model\",\n",
    ")\n",
    "\n",
    "force_load_diffusers_widget = widgets.Dropdown(\n",
    "    options=force_load_diffusers_options,\n",
    "    value=globals().get(\"force_load_diffusers\", False),\n",
    "    description=\"force_load_diffusers\",\n",
    ")\n",
    "\n",
    "resolution_widget = widgets.IntText(\n",
    "    value=int(globals().get(\"resolution\", 1024)),\n",
    "    description=\"resolution\",\n",
    ")\n",
    "\n",
    "num_repeats_widget = widgets.IntText(\n",
    "    value=int(globals().get(\"num_repeats\", 2)),\n",
    "    description=\"num_repeats\",\n",
    ")\n",
    "\n",
    "how_many_widget = widgets.IntText(\n",
    "    value=int(globals().get(\"how_many\", 40)),\n",
    "    description=\"how_many\",\n",
    ")\n",
    "\n",
    "unet_lr_widget = widgets.Text(\n",
    "    value=_format_scientific(globals().get(\"unet_lr\", 1e-4)),\n",
    "    description=\"unet_lr\",\n",
    ")\n",
    "\n",
    "text_encoder_lr_widget = widgets.Text(\n",
    "    value=_format_scientific(globals().get(\"text_encoder_lr\", 5e-5)),\n",
    "    description=\"text_encoder_lr\",\n",
    ")\n",
    "\n",
    "lr_scheduler_widget = widgets.Dropdown(\n",
    "    options=lr_scheduler_options,\n",
    "    value=_coalesce(globals().get(\"lr_scheduler\", lr_scheduler_default), lr_scheduler_options, lr_scheduler_default),\n",
    "    description=\"lr_scheduler\",\n",
    ")\n",
    "\n",
    "lora_type_widget = widgets.Dropdown(\n",
    "    options=lora_type_options,\n",
    "    value=_coalesce(globals().get(\"lora_type\", lora_type_options[0]), lora_type_options, lora_type_options[0]),\n",
    "    description=\"lora_type\",\n",
    ")\n",
    "\n",
    "network_dim_widget = widgets.IntText(\n",
    "    value=int(globals().get(\"network_dim\", 16)),\n",
    "    description=\"network_dim\",\n",
    ")\n",
    "\n",
    "network_alpha_widget = widgets.IntText(\n",
    "    value=int(globals().get(\"network_alpha\", 32)),\n",
    "    description=\"network_alpha\",\n",
    ")\n",
    "\n",
    "train_batch_size_widget = widgets.IntText(\n",
    "    value=int(globals().get(\"train_batch_size\", 8)),\n",
    "    description=\"train_batch_size\",\n",
    ")\n",
    "\n",
    "precision_default = globals().get(\"precision\", \"fp16\")\n",
    "if precision_default not in precision_options:\n",
    "    if \"bf16\" in str(precision_default):\n",
    "        precision_default = \"mixed bf16\"\n",
    "    elif \"fp16\" in str(precision_default):\n",
    "        precision_default = \"mixed fp16\"\n",
    "    else:\n",
    "        precision_default = precision_options[0]\n",
    "\n",
    "precision_widget = widgets.Dropdown(\n",
    "    options=precision_options,\n",
    "    value=precision_default,\n",
    "    description=\"precision\",\n",
    ")\n",
    "\n",
    "optimizer_default = globals().get(\"optimizer\", \"Prodigy\")\n",
    "if optimizer_default not in optimizer_options:\n",
    "    optimizer_default = \"Prodigy\"\n",
    "\n",
    "optimizer_widget = widgets.Dropdown(\n",
    "    options=optimizer_options,\n",
    "    value=optimizer_default,\n",
    "    description=\"optimizer\",\n",
    ")\n",
    "\n",
    "apply_button = widgets.Button(\n",
    "    description=\"Aplicar par\u00e1metros\",\n",
    "    button_style=\"success\",\n",
    "    icon=\"check\",\n",
    ")\n",
    "\n",
    "status_output = widgets.HTML()\n",
    "\n",
    "def apply_params(_=None):\n",
    "    updates = {\n",
    "        \"project_name\": project_name_widget.value.strip(),\n",
    "        \"training_model\": training_model_widget.value,\n",
    "        \"force_load_diffusers\": bool(force_load_diffusers_widget.value),\n",
    "        \"resolution\": int(resolution_widget.value),\n",
    "        \"num_repeats\": int(num_repeats_widget.value),\n",
    "        \"how_many\": int(how_many_widget.value),\n",
    "        \"unet_lr\": float(unet_lr_widget.value),\n",
    "        \"text_encoder_lr\": float(text_encoder_lr_widget.value),\n",
    "        \"lr_scheduler\": lr_scheduler_widget.value,\n",
    "        \"lora_type\": lora_type_widget.value,\n",
    "        \"network_dim\": int(network_dim_widget.value),\n",
    "        \"network_alpha\": int(network_alpha_widget.value),\n",
    "        \"train_batch_size\": int(train_batch_size_widget.value),\n",
    "        \"precision\": precision_widget.value,\n",
    "        \"optimizer\": optimizer_widget.value,\n",
    "    }\n",
    "    globals().update(updates)\n",
    "    unet_lr_widget.value = _format_scientific(updates[\"unet_lr\"])\n",
    "    text_encoder_lr_widget.value = _format_scientific(updates[\"text_encoder_lr\"])\n",
    "    status_output.value = \"<b>Par\u00e1metros actualizados.</b>\"\n",
    "\n",
    "apply_button.on_click(apply_params)\n",
    "\n",
    "form_items = widgets.VBox([\n",
    "    project_name_widget,\n",
    "    training_model_widget,\n",
    "    force_load_diffusers_widget,\n",
    "    resolution_widget,\n",
    "    num_repeats_widget,\n",
    "    how_many_widget,\n",
    "    unet_lr_widget,\n",
    "    text_encoder_lr_widget,\n",
    "    lr_scheduler_widget,\n",
    "    lora_type_widget,\n",
    "    network_dim_widget,\n",
    "    network_alpha_widget,\n",
    "    train_batch_size_widget,\n",
    "    precision_widget,\n",
    "    optimizer_widget,\n",
    "    apply_button,\n",
    "    status_output,\n",
    "])\n",
    "\n",
    "display(Markdown(\"### Configuraci\u00f3n r\u00e1pida del entrenamiento\"))\n",
    "display(form_items)\n",
    "\n",
    "apply_params()\n",
    "\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "AT1WRbhiHRCJ"
   },
   "outputs": [],
   "source": [
    "#lEE todos los parametros y modificalos segun tu entrenamiento\n",
    "#pon el mismo nombre del lora que usaste en la celda de arriba, en el parametro project_name\n",
    "#una vez todo hecho y subido tu dataset, solo inicia esta celda y se ejecutar\u00e1 el entrenador.\n",
    "import os, re, sys, toml\n",
    "from pathlib import Path\n",
    "from time import time\n",
    "import time\n",
    "from IPython.display import Markdown, display, HTML, clear_output\n",
    "from huggingface_hub.utils import disable_progress_bars\n",
    "import logging\n",
    "\n",
    "root_dir = \"/teamspace/studios/this_studio\"\n",
    "trainer_dir = os.path.join(root_dir, \"LoRA_Easy_Training_scripts_Backend\")\n",
    "kohya_dir = os.path.join(trainer_dir, \"sd-scripts\")\n",
    "models_dir = \"/teamspace/studios/this_studio/models\"\n",
    "downloads_dir = os.path.join(root_dir, \"downloads\")\n",
    "custom_optimizer_path = os.path.join(trainer_dir, \"custom_scheduler\")\n",
    "if custom_optimizer_path not in sys.path:\n",
    "  sys.path.append(custom_optimizer_path)\n",
    "os.environ[\"PYTHONPATH\"] = custom_optimizer_path + os.pathsep + os.environ.get(\"PYTHONPATH\", \"\")\n",
    "\n",
    "# Lightning notebooks run continuously; automatic shutdown is not managed here.\n",
    "print(\"\ud83d\udd35 Lightning environment detectado. Det\u00e9n el cuaderno manualmente cuando termines.\")\n",
    "\n",
    "# These carry information from past executions\n",
    "if \"model_url\" in globals():\n",
    "  old_model_url = model_url\n",
    "else:\n",
    "  old_model_url = None\n",
    "if \"dependencies_installed\" not in globals():\n",
    "  dependencies_installed = False\n",
    "if \"model_file\" not in globals():\n",
    "  model_file = None\n",
    "\n",
    "# These may be set by other cells, some are legacy\n",
    "if \"custom_dataset\" not in globals():\n",
    "  custom_dataset = None\n",
    "if \"override_dataset_config_file\" not in globals():\n",
    "  override_dataset_config_file = None\n",
    "if \"override_config_file\" not in globals():\n",
    "  override_config_file = None\n",
    "\n",
    "COMMIT = \"fa2427c6b468231e8e270e40fe72add780118dbe\"\n",
    "LOWRAM = False\n",
    "LOAD_TRUNCATED_IMAGES = True\n",
    "BETTER_EPOCH_NAMES = True\n",
    "FIX_DIFFUSERS = True\n",
    "FIX_WANDB_WARNING = True\n",
    "\n",
    "#@title ## \ud83d\udea9 Start Here\n",
    "\n",
    "#@markdown ### \u25b6\ufe0f Setup\n",
    "#@markdown El nombre de tu proyecto ser\u00e1 el mismo que el de la carpeta que contiene tus im\u00e1genes. No se permiten espacios, puedes usar `gui\u00f3n bajo` si el nombre es muy largo.\n",
    "project_name_param = \" \" #@param {type:\"string\"}\n",
    "project_name = globals().get(\"project_name\", project_name_param).strip()\n",
    "#@markdown La estructura de carpetas no importa y es puramente por comodidad. Aseg\u00farate de elegir siempre el mismo.  Me gusta organizar por proyecto.\n",
    "folder_structure = \"Organize by project (lora_projects/project_name/dataset)\" #@param [\"Organize by category (lora_training/datasets/project_name)\", \"Organize by project (lora_projects/project_name/dataset)\"]\n",
    "#@markdown Decida el modelo que se descargar\u00e1 y utilizar\u00e1 para el entrenamiento. Tambi\u00e9n puedes elegir tu propio modelo pegando su enlace de descarga o proporcionando una ruta dentro de `/teamspace/studios/this_studio`.\n",
    "training_model_param = \"Illustrious_2.0\" # @param [\"Pony Diffusion V6 XL\",\"Animagine XL V3\",\"animagine_4.0_zero\",\"Illustrious_0.1\",\"Illustrious_2.0\",\"NoobAI-XL0.75\",\"NoobAI-XL0.5\",\"Stable Diffusion XL 1.0 base\",\"NoobAIXL0_75vpred\",\"RouWei_v080vpred\"]\n",
    "training_model = globals().get(\"training_model\", training_model_param)\n",
    "optional_custom_training_model = \"\" #@param {type:\"string\"}\n",
    "#@markdown Esto forzara el uso del modelo en formato diffusers, puede ser util en ciertos casos. <p>\n",
    "#@markdown Manten esto desmarcado para usar un modelo ckpt (.safetensors) para el entrenamiento.\n",
    "force_load_diffusers_param = False # @param {\"type\":\"boolean\"}\n",
    "force_load_diffusers = globals().get(\"force_load_diffusers\", force_load_diffusers_param)\n",
    "#@markdown Marca est\u00e1 opci\u00f3n si el modelo custom esta en dicho formato\n",
    "custom_model_is_diffusers = False #@param {type:\"boolean\"}\n",
    "#@markdown Marca esta opci\u00f3n si tu modelo soporta vpred de lo contrario dejala desmarcada.\n",
    "custom_model_is_vpred = False #@param {type:\"boolean\"}\n",
    "#@markdown Utilice wandb si desea visualizar el progreso de su entrenamiento a lo largo del tiempo.\n",
    "wandb_key = \"\" #@param {type:\"string\"}\n",
    "\n",
    "load_diffusers = (custom_model_is_diffusers and len(optional_custom_training_model) > 0) \\\n",
    "                 or force_load_diffusers\n",
    "vpred = custom_model_is_vpred and len(optional_custom_training_model) > 0\n",
    "\n",
    "if optional_custom_training_model:\n",
    "  model_url = optional_custom_training_model\n",
    "elif \"Pony\" in training_model:\n",
    "  if load_diffusers:\n",
    "    model_url = \"https://huggingface.co/WhiteAiZ/Pony_diffusion_v6_diffusers_fp16\"\n",
    "  else:\n",
    "    model_url = \"https://huggingface.co/WhiteAiZ/PonyXL/resolve/main/PonyDiffusionV6XL.safetensors\"\n",
    "  model_file = os.path.join(models_dir, \"ponyDiffusionV6XL.safetensors\")\n",
    "elif \"Animagine\" in training_model:\n",
    "  if load_diffusers:\n",
    "    model_url = \"https://huggingface.co/cagliostrolab/animagine-xl-3.0\"\n",
    "  else:\n",
    "    model_url = \"https://civitai.com/api/download/models/293564\"\n",
    "  model_file = os.path.join(models_dir, \"animagineXLV3.safetensors\")\n",
    "elif \"animagine_4.0_zero\" in training_model:\n",
    "  if load_diffusers:\n",
    "    model_url = \"https://huggingface.co/cagliostrolab/animagine-xl-4.0-zero\"\n",
    "  else:\n",
    "    model_url = \"https://huggingface.co/cagliostrolab/animagine-xl-4.0-zero/resolve/main/animagine-xl-4.0-zero.safetensors\"\n",
    "  model_file = os.path.join(models_dir, \"animagine-xl-4.0-zero.safetensors\")\n",
    "elif \"Illustrious_0.1\" in training_model:\n",
    "  if load_diffusers:\n",
    "    model_url = \"https://huggingface.co/OnomaAIResearch/Illustrious-xl-early-release-v0\"\n",
    "  else:\n",
    "    model_url = \"https://huggingface.co/OnomaAIResearch/Illustrious-xl-early-release-v0/resolve/main/Illustrious-XL-v0.1.safetensors\"\n",
    "elif \"Illustrious_2.0\" in training_model:\n",
    "  if load_diffusers:\n",
    "    model_url = \"https://huggingface.co/WhiteAiZ/Illustrious_2.0\"\n",
    "  else:\n",
    "    model_url = \"https://huggingface.co/WhiteAiZ/Illustrious_2.0/resolve/main/illustriousXL20_v20.safetensors\"\n",
    "  model_file = os.path.join(models_dir, \"illustriousXL20_v20.safetensors\")\n",
    "elif \"NoobAI-XL0.75\" in training_model:\n",
    "  if load_diffusers:\n",
    "    model_url = \"https://huggingface.co/Laxhar/noobai-XL-0.75\"\n",
    "  else:\n",
    "    model_url = \"https://huggingface.co/Laxhar/noobai-XL-0.75/resolve/main/NoobAI-XL-v0.75.safetensors\"\n",
    "elif \"NoobAI-XL0.5\" in training_model:\n",
    "  if load_diffusers:\n",
    "    model_url = \"https://huggingface.co/Laxhar/noobai-XL-0.5\"\n",
    "  else:\n",
    "    model_url = \"https://huggingface.co/Laxhar/noobai-XL-0.5/resolve/main/NoobAI-XL-v0.5.safetensors\"\n",
    "elif \"Stable Diffusion XL 1.0 base\" in training_model:\n",
    "  if load_diffusers:\n",
    "    model_url = \"https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/\"\n",
    "  else:\n",
    "    model_url = \"https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/resolve/main/sd_xl_base_1.0.safetensors\"\n",
    "elif \"NoobAIXL0_75vpred\" in training_model:\n",
    "  vpred = True\n",
    "  if load_diffusers:\n",
    "    model_url = \"https://huggingface.co/Laxhar/noobai-XL-Vpred-0.75\"\n",
    "  else:\n",
    "    model_url = \"https://huggingface.co/Laxhar/noobai-XL-Vpred-0.75/resolve/main/NoobAI-XL-Vpred-v0.75.safetensors\"\n",
    "  model_file = os.path.join(models_dir, \"NoobAI-XL-Vpred-v0.75.safetensors\")\n",
    "else:\n",
    "  vpred = True\n",
    "  if load_diffusers:\n",
    "    model_url = \"https://huggingface.co/John6666/rouwei-v080-vpred-sdxl\"\n",
    "  else:\n",
    "    model_url = \"https://huggingface.co/WhiteAiZ/RouWei/resolve/main/rouwei_v080Vpred.safetensors\"\n",
    "  model_file = os.path.join(models_dir, \"rouwei_v080Vpred.safetensors\")\n",
    "\n",
    "if load_diffusers:\n",
    "  vae_file= \"stabilityai/sdxl-vae\"\n",
    "else:\n",
    "  vae_url = \"https://huggingface.co/stabilityai/sdxl-vae/resolve/main/sdxl_vae.safetensors\"\n",
    "  vae_file = os.path.join(models_dir, \"sdxl_vae.safetensors\")\n",
    "\n",
    "model_url = model_url.strip()\n",
    "\n",
    "#@markdown ### \u25b6\ufe0f Processing\n",
    "#@markdown Por defecto la resoluci\u00f3n para personajes es 1024. otras resoluciones que puedes usar son 896 (recomendado para personajes o 1024) y 768 (recomendado para estilos, puedes usar m\u00e1s repeticiones con esta resoluci\u00f3n).\n",
    "resolution_param = 1024 #@param {type:\"dropdown\", min:768, max:1536, step:128}\n",
    "resolution = globals().get(\"resolution\", resolution_param)\n",
    "#@markdown Activa `Flip Aug`si tu dataset es peque\u00f1o, util en personajes isometricos, volteara todas tus imagenes (modo espejo) para aprender el doble, pero podria afectar a personajes con tatuajes, marcas, cicatrices etc...\n",
    "flip_aug = False #@param {type:\"boolean\"}\n",
    "caption_extension = \".txt\" # @param [\".txt\",\".caption\"]\n",
    "#@markdown Mezcla etiquetas de anime, mejora el aprendizaje y las indicaciones.  Una etiqueta de activaci\u00f3n va al comienzo de cada archivo de texto y no se mezclar\u00e1.<p>\n",
    "shuffle_tags = True #@param {type:\"boolean\"}\n",
    "shuffle_caption = shuffle_tags\n",
    "activation_tags = \"1\" #@param [0,1,2,3]\n",
    "keep_tokens = int(activation_tags)\n",
    "\n",
    "#@markdown ### \u25b6\ufe0f Steps <p>\n",
    "#@markdown Tus im\u00e1genes se repetir\u00e1n esta cantidad de veces durante el entrenamiento. Te recomiendo que tus im\u00e1genes multiplicadas por sus repeticiones est\u00e9 entre 200 y 400.\n",
    "num_repeats_param = 2 #@param {type:\"number\"}\n",
    "num_repeats = globals().get(\"num_repeats\", num_repeats_param)\n",
    "#@markdown Elige cu\u00e1nto tiempo quieres entrenar.  Un buen punto de partida es alrededor de 10 \u00e9pocas o alrededor de 2000 pasos.<p>\n",
    "#@markdown Una \u00e9poca es una cantidad de pasos igual a: la cantidad de im\u00e1genes multiplicada por sus repeticiones, dividida por el tama\u00f1o del lote. <p>\n",
    "preferred_unit = \"Epochs\" #@param [\"Epochs\", \"Steps\"]\n",
    "how_many_param = 40 #@param {type:\"number\"}\n",
    "how_many = globals().get(\"how_many\", how_many_param)\n",
    "max_train_epochs = how_many if preferred_unit == \"Epochs\" else None\n",
    "max_train_steps = how_many if preferred_unit == \"Steps\" else None\n",
    "#@markdown Guardar m\u00e1s \u00e9pocas te permitir\u00e1 comparar mejor el progreso de tu Lora.\n",
    "save_every_n_epochs = 1 #@param {type:\"number\"}\n",
    "keep_only_last_n_epochs = 5 #@param {type:\"number\"}\n",
    "if not save_every_n_epochs:\n",
    "  save_every_n_epochs = max_train_epochs\n",
    "if not keep_only_last_n_epochs:\n",
    "  keep_only_last_n_epochs = max_train_epochs\n",
    "\n",
    "#@markdown ### \u25b6\ufe0f Learning\n",
    "#@markdown La tasa de aprendizaje es lo m\u00e1s importante para tus resultados. Si quieres entrenar m\u00e1s lento con muchas im\u00e1genes, o si tu dim y alfa son altos, mueve el unet a 2e-4 o menos.  <p>\n",
    "#@markdown El codificador de texto ayuda al Lora a aprender conceptos un poco mejor.  Se recomienda hacerlo la mitad o una quinta parte del unet.  Si est\u00e1s entrenando un estilo, puedes incluso configurarlo en 0.\n",
    "unet_lr_param = 1e-4 #@param {type:\"number\"}\n",
    "unet_lr = globals().get(\"unet_lr\", unet_lr_param)\n",
    "text_encoder_lr_param = 5e-5 #@param {type:\"number\"}\n",
    "text_encoder_lr = globals().get(\"text_encoder_lr\", text_encoder_lr_param)\n",
    "#@markdown El scheduler es el algoritmo que gu\u00eda la tasa de aprendizaje. Si no est\u00e1 seguro, elije \"constant\" e ignore el n\u00famero. Personalmente recomiendo `cosine_with_restarts` con 3 reinicios.\n",
    "lr_scheduler_param = \"constant_with_warmup\" # @param [\"constant\",\"cosine\",\"cosine_with_restarts\",\"constant_with_warmup\",\"linear\",\"polynomial\",\"rex\"]\n",
    "lr_scheduler = globals().get(\"lr_scheduler\", lr_scheduler_param)\n",
    "lr_scheduler_number = 0 #@param {type:\"number\"}\n",
    "#@markdown Pasos dedicados a \"calentar\" la tasa de aprendizaje durante la capacitaci\u00f3n para lograr eficiencia. Recomiendo dejarlo al 5%.\n",
    "lr_warmup_ratio = 0.05 #@param {type:\"slider\", min:0.0, max:0.2, step:0.01}\n",
    "lr_warmup_steps = 100 #@param {type:\"number\"}\n",
    "#@markdown Estas configuraciones pueden producir mejores resultados.`min_snr_gamma` ajusta la p\u00e9rdida con el tiempo. `ip_noise_gamma` ajusta el ruido aleatorio.\n",
    "min_snr_gamma_enabled = True #@param {type:\"boolean\"}\n",
    "min_snr_gamma = 8.0 #@param {type:\"slider\", min:4, max:16.0, step:0.5}\n",
    "ip_noise_gamma_enabled = True #@param {type:\"boolean\"}\n",
    "ip_noise_gamma = 0.05 #@param {type:\"slider\", min:0.05, max:0.1, step:0.01}\n",
    "#@markdown Multinoise puede ayudar con el equilibrio del color (negros m\u00e1s oscuros, blancos m\u00e1s claros) no es necesario activarlo si entrenas Lora Vpred.\n",
    "multinoise = False #@param {type:\"boolean\"}\n",
    "\n",
    "#@markdown ### \u25b6\ufe0f Structure\n",
    "#@markdown LoRA es del tipo cl\u00e1sico y bueno para una variedad de prop\u00f3sitos. LoCon es bueno con los estilos art\u00edsticos (tambi\u00e9n funciona con personajes) ya que tiene m\u00e1s capas para aprender m\u00e1s aspectos del conjunto de datos.\n",
    "lora_type_param = \"LoRA\" # @param [\"LoRA\",\"LoCon\"]\n",
    "lora_type = globals().get(\"lora_type\", lora_type_param)\n",
    "\n",
    "#@markdown A continuaci\u00f3n se muestran algunos valores XL recomendados para las siguientes configuraciones:\n",
    "\n",
    "#@markdown | type | network_dim | network_alpha | conv_dim | conv_alpha |\n",
    "#@markdown | :---: | :---: | :---: | :---: | :---: |\n",
    "#@markdown | Personaje LoRA | 4 | 16 |   |   |\n",
    "#@markdown | Regular y Estilo LoRA | 8 | 4 |   |   |\n",
    "#@markdown | Style LoCon | 16 | 8 | 16 | 8 |\n",
    "\n",
    "#@markdown M\u00e1s dim significa un Lora m\u00e1s grande, puede contener m\u00e1s informaci\u00f3n, pero m\u00e1s no siempre es mejor.\n",
    "network_dim_param = 16 #@param {type:\"number\", min:1, max:32, step:1}\n",
    "network_dim = globals().get(\"network_dim\", network_dim_param)\n",
    "network_alpha_param = 32 #@param {type:\"number\", min:1, max:32, step:1}\n",
    "network_alpha = globals().get(\"network_alpha\", network_alpha_param)\n",
    "#@markdown Los siguientes dos valores solo se aplican a las capas adicionales de LoCon.\n",
    "conv_dim = 16 #@param {type:\"number\", min:1, max:32, step:1}\n",
    "conv_alpha = 8 #@param {type:\"number\", min:1, max:32, step:1}\n",
    "\n",
    "network_module = \"networks.lora\"\n",
    "network_args = None\n",
    "if lora_type.lower() == \"locon\":\n",
    "  network_args = [f\"conv_dim={conv_dim}\", f\"conv_alpha={conv_alpha}\"]\n",
    "\n",
    "#@markdown ### \u25b6\ufe0f Training\n",
    "#@markdown Ajuste estos par\u00e1metros seg\u00fan la configuraci\u00f3n de su colab.\n",
    "\n",
    "#@markdown El batch size de 4 es el predeterminado pero puedes incrementarlo incluso a 8 usando una resoluci\u00f3n baja (768).\n",
    "#@markdown\n",
    "#@markdown Un tama\u00f1o de lote m\u00e1s alto suele ser m\u00e1s r\u00e1pido pero utiliza m\u00e1s memoria.\n",
    "train_batch_size_param = 8 #@param {type:\"slider\", min:1, max:16, step:1}\n",
    "train_batch_size = globals().get(\"train_batch_size\", train_batch_size_param)\n",
    "#@markdown xformers funciona mejor que sdpa con los nuevos scrips.\n",
    "cross_attention = \"xformers\" #@param [\"sdpa\", \"xformers\"]\n",
    "#@markdown Utilice `full fp16` para el uso m\u00ednimo de memoria. <p>\n",
    "#@markdown `float, full bf16, full fp16, mixed bf16 y mixed fp16` solo funcionaran con colab pro. <p>\n",
    "#@markdown El Lora se entrenar\u00e1 con la precisi\u00f3n seleccionada, pero siempre se guardar\u00e1 en formato fp16 por razones de compatibilidad.\n",
    "precision_param = \"fp16\" #@param [\"float\", \"full fp16\", \"full bf16\", \"mixed fp16\", \"mixed bf16\"]\n",
    "precision = globals().get(\"precision\", precision_param)\n",
    "#@markdown El almacenamiento en cach\u00e9 latente en disco agregar\u00e1 un archivo de 250 KB junto a cada imagen, pero usar\u00e1 considerablemente menos memoria.\n",
    "cache_latents = True #@param {type:\"boolean\"}\n",
    "cache_latents_to_disk = False #@param {type:\"boolean\"}\n",
    "#@markdown La siguiente opci\u00f3n desactivar\u00e1 shuffle_tags y deshabilitar\u00e1 el entrenamiento del codificador de texto.\n",
    "cache_text_encoder_outputs  = False  # @param {type:\"boolean\"}\n",
    "\n",
    "mixed_precision = \"no\"\n",
    "if \"fp16\" in precision:\n",
    "  mixed_precision = \"fp16\"\n",
    "elif \"bf16\" in precision:\n",
    "  mixed_precision = \"bf16\"\n",
    "full_precision = \"full\" in precision\n",
    "\n",
    "#@markdown ### \u25b6\ufe0f Advanced\n",
    "#@markdown El optimizador es el algoritmo utilizado para el entrenamiento. Adafactor es el predeterminado y funciona muy bien, mientras que el Prodigy administra la tasa de aprendizaje autom\u00e1ticamente y puede tener varias ventajas, como entrenar m\u00e1s r\u00e1pido, debido a que necesita menos pasos y funcionan mejor para datasets peque\u00f1os.\n",
    "optimizer_param = \"Prodigy\" #@param [\"AdamW8bit\", \"Prodigy\", \"DAdaptation\", \"DadaptAdam\", \"DadaptLion\", \"AdamW\", \"Lion\", \"SGDNesterov\", \"SGDNesterov8bit\", \"AdaFactor\", \"Came\"]\n",
    "optimizer = globals().get(\"optimizer\", optimizer_param)\n",
    "#@markdown Argumentos recomendados para Adafactor: `scale_parameter=False relative_step=False warmup_init=False` <p>\n",
    "#@markdown Argumentos recomendados para AdamW8bit: `weight_decay=0.1 betas=[0.9,0.99]`<p>\n",
    "#@markdown Argumentos recomendados para Prodigy: `decouple=True weight_decay=0.01 betas=[0.9,0.999] d_coef=2 use_bias_correction=True safeguard_warmup=True`<p>\n",
    "#@markdown Argumentos recomendado para CAME: `weight_decay=0.04` <p>\n",
    "#@markdown Si se selecciona Dadapt o Prodigy y se marca la casilla recomendada, los siguientes valores recomendados anular\u00e1n cualquier configuraci\u00f3n anterior:<p>\n",
    "#@markdown `unet_lr=0.75`, `text_encoder_lr=0.75`, `network_alpha=network_dim`, `full_precision=True`<p>\n",
    "#@markdown Si selecciona Prodigy o Dadapt recomiendo usar `mixed fp16`para mejores resultados. <p>\n",
    "recommended_values = True #@param {type:\"boolean\"}\n",
    "#@markdown Alternativamente, establezca sus propios argumentos de optimizador separados por espacios (no comas). `recommended_values` debe estar deshabilitado.\n",
    "optimizer_args = \"\" #@param {type:\"string\"}\n",
    "optimizer_args = [a.strip() for a in optimizer_args.split(' ') if a]\n",
    "\n",
    "\n",
    "if recommended_values:\n",
    "  if any(opt in optimizer.lower() for opt in [\"dadapt\", \"prodigy\"]):\n",
    "    unet_lr = 0.75\n",
    "    text_encoder_lr = 0.75\n",
    "    network_alpha = network_dim\n",
    "    full_precision = False\n",
    "  if optimizer == \"Prodigy\":\n",
    "    optimizer_args = [\"decouple=True\", \"weight_decay=0.01\", \"betas=[0.9,0.999]\", \"d_coef=2\", \"use_bias_correction=True\", \"safeguard_warmup=True\"]\n",
    "  elif optimizer == \"AdamW8bit\":\n",
    "    optimizer_args = [\"weight_decay=0.1\", \"betas=[0.9,0.99]\"]\n",
    "  elif optimizer == \"AdaFactor\":\n",
    "    optimizer_args = [\"scale_parameter=False\", \"relative_step=False\", \"warmup_init=False\"]\n",
    "  elif optimizer == \"Came\":\n",
    "    optimizer_args = [\"weight_decay=0.04\"]\n",
    "\n",
    "if optimizer == \"Came\":\n",
    "  optimizer = \"LoraEasyCustomOptimizer.came.CAME\"\n",
    "\n",
    "lr_scheduler_type = None\n",
    "lr_scheduler_args = None\n",
    "lr_scheduler_num_cycles = lr_scheduler_number\n",
    "lr_scheduler_power = lr_scheduler_number\n",
    "\n",
    "if \"rex\" in lr_scheduler:\n",
    "  lr_scheduler = \"cosine\"\n",
    "  lr_scheduler_type = \"LoraEasyCustomOptimizer.RexAnnealingWarmRestarts.RexAnnealingWarmRestarts\"\n",
    "  lr_scheduler_args = [\"min_lr=1e-9\", \"gamma=0.9\", \"d=0.9\"]\n",
    "\n",
    "# Misc\n",
    "seed = 42\n",
    "gradient_accumulation_steps = 1\n",
    "bucket_reso_steps = 64\n",
    "min_bucket_reso = 256\n",
    "max_bucket_reso = 4096\n",
    "\n",
    "#@markdown ### \u25b6\ufe0f Ready\n",
    "#@markdown Ahora puedes ejecutar esta celda para entrenar tu Lora. \u00a1Buena suerte! <p>\n",
    "\n",
    "# \ud83d\udc69\u200d\ud83d\udcbb Cool code goes here\n",
    "\n",
    "\n",
    "for required_dir in (models_dir, downloads_dir):\n",
    "  os.makedirs(required_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "def lightning_rel(path):\n",
    "  try:\n",
    "    return os.path.relpath(path, root_dir)\n",
    "  except ValueError:\n",
    "    return path\n",
    "\n",
    "\n",
    "venv_python = \"/home/zeus/miniconda3/envs/cloudspace/bin/python3\"\n",
    "#venv_pip = os.path.join(kohya_dir, \"venv/bin/pip\")\n",
    "train_network = os.path.join(kohya_dir, \"sdxl_train_network.py\")\n",
    "\n",
    "if \"lora_projects\" in folder_structure:\n",
    "  main_dir      = os.path.join(root_dir, \"lora_projects\")\n",
    "  log_folder    = os.path.join(main_dir, \"_logs\")\n",
    "  config_folder = os.path.join(main_dir, project_name)\n",
    "  images_folder = os.path.join(main_dir, project_name, \"dataset\")\n",
    "  output_folder = os.path.join(main_dir, project_name, \"output\")\n",
    "else:\n",
    "  main_dir      = os.path.join(root_dir, \"lora_training\")\n",
    "  images_folder = os.path.join(main_dir, \"datasets\", project_name)\n",
    "  output_folder = os.path.join(main_dir, \"output\", project_name)\n",
    "  config_folder = os.path.join(main_dir, \"config\", project_name)\n",
    "  log_folder    = os.path.join(main_dir, \"log\")\n",
    "\n",
    "config_file = os.path.join(config_folder, \"training_config.toml\")\n",
    "dataset_config_file = os.path.join(config_folder, \"dataset_config.toml\")\n",
    "\n",
    "def install_trainer():\n",
    "  global installed\n",
    "  libtcmalloc_path = os.path.join(root_dir, \"libtcmalloc_minimal.so.4\")\n",
    "\n",
    "  if 'installed' not in globals():\n",
    "    installed = False\n",
    "\n",
    "  if not os.path.exists(libtcmalloc_path):\n",
    "    !wget -q -c --show-progress https://github.com/camenduru/gperftools/releases/download/v1.0/libtcmalloc_minimal.so.4 -O {libtcmalloc_path}\n",
    "\n",
    "  if not os.path.exists(trainer_dir):\n",
    "    !git clone -b dev https://github.com/gwhitez/LoRA_Easy_Training_scripts_Backend.git {trainer_dir}\n",
    "  else:\n",
    "    os.chdir(trainer_dir)\n",
    "    !git pull\n",
    "    os.chdir(root_dir)\n",
    "\n",
    "  os.chdir(trainer_dir)\n",
    "  display(HTML(\"<h2 style='color: yellow;'>Descargando dependencias</h2>\"))\n",
    "  !chmod 755 /teamspace/studios/this_studio/LoRA_Easy_Training_scripts_Backend/colab_install.sh\n",
    "  !/teamspace/studios/this_studio/LoRA_Easy_Training_scripts_Backend/colab_install.sh > install_log.txt 2>&1\n",
    "\n",
    "  os.chdir(kohya_dir)\n",
    "  if LOAD_TRUNCATED_IMAGES:\n",
    "    !sed -i 's/from PIL import Image/from PIL import Image, ImageFile\\nImageFile.LOAD_TRUNCATED_IMAGES=True/g' library/train_util.py\n",
    "  if BETTER_EPOCH_NAMES:\n",
    "    !sed -i 's/{:06d}/{:02d}/g' library/train_util.py\n",
    "    !sed -i 's/\".\" + args.save_model_as)/\"-{:02d}.\".format(num_train_epochs) + args.save_model_as)/g' train_network.py\n",
    "  if FIX_DIFFUSERS:\n",
    "    deprecation_utils = os.path.join(kohya_dir, \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/diffusers/utils/deprecation_utils.py\")\n",
    "    !sed -i 's/if version.parse/if False:#/g' {deprecation_utils}\n",
    "  if FIX_WANDB_WARNING:\n",
    "    !sed -i 's/accelerator.log(logs, step=epoch + 1)//g' train_network.py\n",
    "    !sed -i 's/accelerator.log(logs, step=epoch + 1)//g' sdxl_train.py\n",
    "\n",
    "  os.environ[\"LD_PRELOAD\"] = libtcmalloc_path\n",
    "  os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "  os.environ[\"BITSANDBYTES_NOWELCOME\"] = \"1\"\n",
    "  os.environ[\"SAFETENSORS_FAST_GPU\"] = \"1\"\n",
    "  os.environ[\"PYTHONWARNINGS\"] = \"ignore\"\n",
    "  os.chdir(root_dir)\n",
    "\n",
    "def validate_dataset():\n",
    "  global lr_warmup_steps, lr_warmup_ratio, caption_extension, keep_tokens, model_url\n",
    "  supported_types = (\".png\", \".jpg\", \".jpeg\", \".webp\", \".bmp\")\n",
    "\n",
    "  print(\"\\n\ud83d\udcbf Checking dataset...\")\n",
    "  if not project_name.strip() or any(c in project_name for c in \" .()\\\"'\\\\/\"):\n",
    "    print(\"\ud83d\udca5 Error: Elija un nombre de proyecto v\u00e1lido.\")\n",
    "    return\n",
    "\n",
    "  # Find the folders and files\n",
    "  if custom_dataset:\n",
    "    try:\n",
    "      datconf = toml.loads(custom_dataset)\n",
    "      datasets = [d for d in datconf[\"datasets\"][0][\"subsets\"]]\n",
    "    except:\n",
    "      print(f\"\ud83d\udca5 Error: El conjunto de datos personalizado no es v\u00e1lido o contiene un error. Por favor, compruebe la plantilla original.\")\n",
    "      return\n",
    "    reg = [d.get(\"image_dir\") for d in datasets if d.get(\"is_reg\", False)]\n",
    "    datasets_dict = {d[\"image_dir\"]: d[\"num_repeats\"] for d in datasets}\n",
    "    folders = datasets_dict.keys()\n",
    "    files = [f for folder in folders for f in os.listdir(folder)]\n",
    "    images_repeats = {folder: (len([f for f in os.listdir(folder) if f.lower().endswith(supported_types)]), datasets_dict[folder]) for folder in folders}\n",
    "  else:\n",
    "    reg = []\n",
    "    folders = [images_folder]\n",
    "    files = os.listdir(images_folder)\n",
    "    images_repeats = {images_folder: (len([f for f in files if f.lower().endswith(supported_types)]), num_repeats)}\n",
    "\n",
    "  # Validation\n",
    "  for folder in folders:\n",
    "    if not os.path.exists(folder):\n",
    "      print(f\"\ud83d\udca5 Error: La carpeta {lightning_rel(folder)} no existe.\")\n",
    "      return\n",
    "  for folder, (img, rep) in images_repeats.items():\n",
    "    if not img:\n",
    "      print(f\"\ud83d\udca5 Error: t\u00fa {lightning_rel(folder)} La carpeta est\u00e1 vac\u00eda.\")\n",
    "      return\n",
    "  test_files = []\n",
    "  for f in files:\n",
    "    if not f.lower().endswith((caption_extension, \".npz\")) and not f.lower().endswith(supported_types):\n",
    "      print(f\"\ud83d\udca5 Error: Archivo no v\u00e1lido en el conjunto de datos: \\\"{f}\\\". Abortar.\")\n",
    "      return\n",
    "    for ff in test_files:\n",
    "      if f.endswith(supported_types) and ff.endswith(supported_types) \\\n",
    "          and os.path.splitext(f)[0] == os.path.splitext(ff)[0]:\n",
    "        print(f\"\ud83d\udca5 Error: Los archivos {f} y {ff} no puede tener el mismo nombre. Abortar.\")\n",
    "        return\n",
    "    test_files.append(f)\n",
    "\n",
    "  if caption_extension and not [txt for txt in files if txt.lower().endswith(caption_extension)]:\n",
    "    caption_extension = \"\"\n",
    "\n",
    "  # Show estimations to the user\n",
    "\n",
    "  pre_steps_per_epoch = sum(img*rep for (img, rep) in images_repeats.values())\n",
    "  steps_per_epoch = pre_steps_per_epoch/train_batch_size\n",
    "  total_steps = max_train_steps or int(max_train_epochs*steps_per_epoch)\n",
    "  estimated_epochs = int(total_steps/steps_per_epoch)\n",
    "  lr_warmup_steps = int(total_steps*lr_warmup_ratio)\n",
    "\n",
    "  for folder, (img, rep) in images_repeats.items():\n",
    "    print(\"\ud83d\udcc1\" + lightning_rel(folder) + (\" (Regularization)\" if folder in reg else \"\"))\n",
    "    print(f\"\ud83d\udcc8 Se encontr\u00f3 {img} im\u00e1genes con {rep} repeticiones, igual {img*rep} pasos.\")\n",
    "  print(f\"\ud83d\udcc9 Divide {pre_steps_per_epoch} pasos por {train_batch_size} batch size para obtener {steps_per_epoch} pasos por epoch.\")\n",
    "  if max_train_epochs:\n",
    "    print(f\"\ud83d\udd2e Habr\u00e1 {max_train_epochs} epochs, por alrededor de {total_steps} total de pasos.\")\n",
    "  else:\n",
    "    print(f\"\ud83d\udd2e Habr\u00e1 {total_steps} pasos, divididos en {estimated_epochs} epochs y algo m\u00e1s.\")\n",
    "\n",
    "  if total_steps > 10000:\n",
    "    print(\"\ud83d\udca5 Error: El total de pasos es demasiado alto. Probablemente cometiste un error. Abortar...\")\n",
    "    return\n",
    "\n",
    "  return True\n",
    "\n",
    "def create_config():\n",
    "  global dataset_config_file, config_file, model_file\n",
    "\n",
    "  if override_config_file:\n",
    "    config_file = override_config_file\n",
    "    print(f\"\\n\u2b55 Using custom config file {config_file}\")\n",
    "  else:\n",
    "    config_dict = {\n",
    "      \"network_arguments\": {\n",
    "        \"unet_lr\": unet_lr,\n",
    "        \"text_encoder_lr\": text_encoder_lr if not cache_text_encoder_outputs else 0,\n",
    "        \"network_dim\": network_dim,\n",
    "        \"network_alpha\": network_alpha,\n",
    "        \"network_module\": network_module,\n",
    "        \"network_args\": network_args,\n",
    "        \"network_train_unet_only\": text_encoder_lr == 0 or cache_text_encoder_outputs,\n",
    "      },\n",
    "      \"optimizer_arguments\": {\n",
    "        \"learning_rate\": unet_lr,\n",
    "        \"lr_scheduler\": lr_scheduler,\n",
    "        \"lr_scheduler_type\": lr_scheduler_type,\n",
    "        \"lr_scheduler_args\": lr_scheduler_args,\n",
    "        \"lr_scheduler_num_cycles\": lr_scheduler_num_cycles if lr_scheduler == \"cosine_with_restarts\" else None,\n",
    "        \"lr_scheduler_power\": lr_scheduler_power if lr_scheduler == \"polynomial\" else None,\n",
    "        \"lr_warmup_steps\": lr_warmup_steps if lr_scheduler not in (\"cosine\", \"constant\") else None,\n",
    "        \"optimizer_type\": optimizer,\n",
    "       \"optimizer_args\": optimizer_args or None,\n",
    "        \"loss_type\": \"l2\",\n",
    "        \"max_grad_norm\": 1.0,\n",
    "      },\n",
    "      \"training_arguments\": {\n",
    "        \"lowram\": LOWRAM,\n",
    "        \"pretrained_model_name_or_path\": model_file,\n",
    "        \"vae\": vae_file,\n",
    "        \"max_train_steps\": max_train_steps,\n",
    "        \"max_train_epochs\": max_train_epochs,\n",
    "        \"train_batch_size\": train_batch_size,\n",
    "        \"seed\": seed,\n",
    "        \"max_token_length\": 225,\n",
    "        \"xformers\": cross_attention == \"xformers\",\n",
    "        \"sdpa\": cross_attention == \"sdpa\",\n",
    "        \"min_snr_gamma\": min_snr_gamma if min_snr_gamma_enabled else None,\n",
    "        \"ip_noise_gamma\": ip_noise_gamma if ip_noise_gamma_enabled else None,\n",
    "        \"no_half_vae\": True,\n",
    "        \"gradient_checkpointing\": True,\n",
    "        \"gradient_accumulation_steps\": gradient_accumulation_steps,\n",
    "        \"max_data_loader_n_workers\": 1,\n",
    "        \"persistent_data_loader_workers\": True,\n",
    "        \"mixed_precision\": mixed_precision,\n",
    "        \"full_fp16\": mixed_precision == \"fp16\" and full_precision,\n",
    "        \"full_bf16\": mixed_precision == \"bf16\" and full_precision,\n",
    "        \"cache_latents\": cache_latents,\n",
    "        \"cache_latents_to_disk\": cache_latents_to_disk,\n",
    "        \"cache_text_encoder_outputs\": cache_text_encoder_outputs,\n",
    "        \"min_timestep\": 0,\n",
    "        \"max_timestep\": 1000,\n",
    "        \"prior_loss_weight\": 1.0,\n",
    "        \"multires_noise_iterations\": 6 if multinoise else None,\n",
    "        \"multires_noise_discount\": 0.3 if multinoise else None,\n",
    "        \"v_parameterization\": vpred or None,\n",
    "        \"scale_v_pred_loss_like_noise_pred\": vpred or None,\n",
    "        \"zero_terminal_snr\": vpred or None,\n",
    "      },\n",
    "      \"saving_arguments\": {\n",
    "        \"save_precision\": \"fp16\",\n",
    "        \"save_model_as\": \"safetensors\",\n",
    "        \"save_every_n_epochs\": save_every_n_epochs,\n",
    "        \"save_last_n_epochs\": keep_only_last_n_epochs,\n",
    "        \"output_name\": project_name,\n",
    "        \"output_dir\": output_folder,\n",
    "        \"log_prefix\": project_name,\n",
    "        \"logging_dir\": log_folder,\n",
    "        \"wandb_api_key\": wandb_key or None,\n",
    "        \"log_with\": \"wandb\" if wandb_key else None,\n",
    "      }\n",
    "    }\n",
    "\n",
    "    for key in config_dict:\n",
    "      if isinstance(config_dict[key], dict):\n",
    "        config_dict[key] = {k: v for k, v in config_dict[key].items() if v is not None}\n",
    "\n",
    "    with open(config_file, \"w\") as f:\n",
    "      f.write(toml.dumps(config_dict))\n",
    "    print(f\"\\n\ud83d\udcc4 Config saved to {config_file}\")\n",
    "\n",
    "  if override_dataset_config_file:\n",
    "    dataset_config_file = override_dataset_config_file\n",
    "    print(f\"\u2b55 Using custom dataset config file {dataset_config_file}\")\n",
    "  else:\n",
    "    dataset_config_dict = {\n",
    "      \"general\": {\n",
    "        \"resolution\": resolution,\n",
    "        \"shuffle_caption\": shuffle_caption and not cache_text_encoder_outputs,\n",
    "        \"keep_tokens\": keep_tokens,\n",
    "        \"flip_aug\": False,\n",
    "        \"caption_extension\": caption_extension,\n",
    "        \"enable_bucket\": True,\n",
    "        \"bucket_no_upscale\": False,\n",
    "        \"bucket_reso_steps\": bucket_reso_steps,\n",
    "        \"min_bucket_reso\": min_bucket_reso,\n",
    "        \"max_bucket_reso\": max_bucket_reso,\n",
    "      },\n",
    "      \"datasets\": toml.loads(custom_dataset)[\"datasets\"] if custom_dataset else [\n",
    "        {\n",
    "          \"subsets\": [\n",
    "            {\n",
    "              \"num_repeats\": num_repeats,\n",
    "              \"image_dir\": images_folder,\n",
    "              \"class_tokens\": None if caption_extension else project_name\n",
    "            }\n",
    "          ]\n",
    "        }\n",
    "      ]\n",
    "    }\n",
    "\n",
    "    for key in dataset_config_dict:\n",
    "      if isinstance(dataset_config_dict[key], dict):\n",
    "        dataset_config_dict[key] = {k: v for k, v in dataset_config_dict[key].items() if v is not None}\n",
    "\n",
    "    with open(dataset_config_file, \"w\") as f:\n",
    "      f.write(toml.dumps(dataset_config_dict))\n",
    "    print(f\"\ud83d\udcc4 Configuraci\u00f3n de dataset guardada en {dataset_config_file}\")\n",
    "\n",
    "def download_model():\n",
    "  global old_model_url, model_url, model_file, vae_url, vae_file\n",
    "\n",
    "  real_model_url = (model_url or \"\").strip()\n",
    "  if not real_model_url:\n",
    "    print(\"\ud83d\udca5 Error: no se especific\u00f3 ning\u00fan modelo base para entrenar.\")\n",
    "    return False\n",
    "\n",
    "  if load_diffusers:\n",
    "    if 'huggingface.co' in real_model_url:\n",
    "      match = re.search(r'huggingface.co/([^/]+)/([^/]+)', real_model_url)\n",
    "      if match:\n",
    "        username = match.group(1)\n",
    "        model_name = match.group(2)\n",
    "        model_file = f\"{username}/{model_name}\"\n",
    "        from huggingface_hub import HfFileSystem\n",
    "        fs = HfFileSystem()\n",
    "        existing_folders = set(fs.ls(model_file, detail=False))\n",
    "        necessary_folders = [\"scheduler\", \"text_encoder\", \"text_encoder_2\", \"tokenizer\", \"tokenizer_2\", \"unet\", \"vae\"]\n",
    "        if all(f\"{model_file}/{folder}\" in existing_folders for folder in necessary_folders):\n",
    "          print(\"\ud83c\udf43 Modelo diffusers identificado; kohya manejar\u00e1 la descarga.\")\n",
    "          return True\n",
    "    raise ValueError(\"\ud83d\udca5 Failed to load Diffusers model. Si este modelo no es diffusers, desactiva la opci\u00f3n correspondiente.\")\n",
    "\n",
    "  local_candidate = None\n",
    "  if '://' not in real_model_url:\n",
    "    candidate = Path(real_model_url)\n",
    "    if not candidate.is_absolute():\n",
    "      candidate = Path(root_dir) / real_model_url.lstrip('/')\n",
    "    if candidate.exists():\n",
    "      local_candidate = candidate\n",
    "    else:\n",
    "      print(f\"\ud83d\udca5 Error: el modelo local {candidate} no existe. Aseg\u00farate de que est\u00e9 dentro de {root_dir} o usa una URL.\")\n",
    "      return False\n",
    "\n",
    "  if local_candidate is not None:\n",
    "    model_file = str(local_candidate)\n",
    "    print(f\"\ud83d\udcc1 Usando modelo local: {model_file}\")\n",
    "  else:\n",
    "    if real_model_url.lower().endswith((\".ckpt\", \".safetensors\")):\n",
    "      filename = os.path.basename(real_model_url)\n",
    "    else:\n",
    "      filename = \"downloaded_model.safetensors\"\n",
    "\n",
    "    civitai_match = re.search(r\"(?:https?://)?(?:www\\.)?civitai\\.com/models/([0-9]+)(/[A-Za-z0-9-_]+)?\", real_model_url)\n",
    "    if civitai_match:\n",
    "      name_hint = civitai_match.group(2)\n",
    "      if name_hint:\n",
    "        filename = f\"{Path(name_hint).name}.safetensors\"\n",
    "      version_match = re.search(r\"modelVersionId=([0-9]+)\", real_model_url)\n",
    "      if version_match:\n",
    "        real_model_url = f\"https://civitai.com/api/download/models/{version_match.group(1)}\"\n",
    "      else:\n",
    "        raise ValueError(\"\ud83d\udca5 optional_custom_training_model contiene un enlace de Civitai sin modelVersionId v\u00e1lido.\")\n",
    "\n",
    "    model_file = os.path.join(models_dir, filename)\n",
    "    if os.path.exists(model_file):\n",
    "      !rm \"{model_file}\"\n",
    "\n",
    "    if re.search(r\"(?:https?://)?(?:www\\.)?huggingface\\.co/[^/]+/[^/]+/blob\", real_model_url):\n",
    "      real_model_url = real_model_url.replace(\"blob\", \"resolve\")\n",
    "\n",
    "    print(f\"\ud83c\udf10 Descargando modelo en {model_file} ...\")\n",
    "    !aria2c \"{real_model_url}\" --console-log-level=warn -c -s 16 -x 16 -k 10M -d {models_dir} -o \"{os.path.basename(model_file)}\"\n",
    "\n",
    "    if not os.path.exists(vae_file):\n",
    "      print(f\"\ud83c\udf10 Descargando VAE en {vae_file} ...\")\n",
    "      !aria2c \"{vae_url}\" --console-log-level=warn -c -s 16 -x 16 -k 10M -d {models_dir} -o \"{os.path.basename(vae_file)}\"\n",
    "\n",
    "  if model_file.lower().endswith(\".safetensors\"):\n",
    "    from safetensors.torch import load_file as load_safetensors\n",
    "    try:\n",
    "      test = load_safetensors(model_file)\n",
    "      del test\n",
    "    except Exception:\n",
    "      new_model_file = os.path.splitext(model_file)[0] + \".ckpt\"\n",
    "      !mv \"{model_file}\" \"{new_model_file}\"\n",
    "      model_file = new_model_file\n",
    "      print(f\"Renombrado modelo a {model_file}\")\n",
    "\n",
    "  if model_file.lower().endswith(\".ckpt\"):\n",
    "    from torch import load as load_ckpt\n",
    "    try:\n",
    "      test = load_ckpt(model_file)\n",
    "      del test\n",
    "    except Exception:\n",
    "      return False\n",
    "\n",
    "  return True\n",
    "\n",
    "\n",
    "def calculate_rex_steps():\n",
    "  # https://github.com/derrian-distro/LoRA_Easy_Training_scripts_Backend/blob/c34084b0435e6e19bb7a01ac1ecbadd185ee8c1e/utils/validation.py#L268\n",
    "  global max_train_steps\n",
    "  print(\"\\n\ud83e\udd14 Calculating Rex steps\")\n",
    "  if max_train_steps:\n",
    "    calculated_max_steps = max_train_steps\n",
    "  else:\n",
    "    from library.train_util import BucketManager\n",
    "    from PIL import Image\n",
    "    from pathlib import Path\n",
    "    import math\n",
    "\n",
    "    with open(dataset_config_file, \"r\") as f:\n",
    "      subsets = toml.load(f)[\"datasets\"][0][\"subsets\"]\n",
    "\n",
    "    supported_types = [\".png\", \".jpg\", \".jpeg\", \".webp\", \".bmp\"]\n",
    "    res = (resolution, resolution)\n",
    "    bucketManager = BucketManager(False, res, min_bucket_reso, max_bucket_reso, bucket_reso_steps)\n",
    "    bucketManager.make_buckets()\n",
    "    for subset in subsets:\n",
    "        for image in Path(subset[\"image_dir\"]).iterdir():\n",
    "            if image.suffix not in supported_types:\n",
    "                continue\n",
    "            with Image.open(image) as img:\n",
    "                bucket_reso, _, _ = bucketManager.select_bucket(img.width, img.height)\n",
    "                for _ in range(subset[\"num_repeats\"]):\n",
    "                    bucketManager.add_image(bucket_reso, image)\n",
    "    steps_before_acc = sum(math.ceil(len(bucket) / train_batch_size) for bucket in bucketManager.buckets)\n",
    "    calculated_max_steps = math.ceil(steps_before_acc / gradient_accumulation_steps) * max_train_epochs\n",
    "    del bucketManager\n",
    "\n",
    "  cycle_steps = calculated_max_steps // (lr_scheduler_num_cycles or 1)\n",
    "  print(f\"  cycle steps: {cycle_steps}\")\n",
    "  lr_scheduler_args.append(f\"first_cycle_max_steps={cycle_steps}\")\n",
    "\n",
    "  warmup_steps = round(calculated_max_steps * lr_warmup_ratio) // (lr_scheduler_num_cycles or 1)\n",
    "  if warmup_steps > 0:\n",
    "    print(f\"  warmup steps: {warmup_steps}\")\n",
    "    lr_scheduler_args.append(f\"warmup_steps={warmup_steps}\")\n",
    "\n",
    "def main():\n",
    "  global dependencies_installed\n",
    "\n",
    "  for dir in (main_dir, trainer_dir, log_folder, images_folder, output_folder, config_folder, models_dir, downloads_dir):\n",
    "    os.makedirs(dir, exist_ok=True)\n",
    "\n",
    "  if not validate_dataset():\n",
    "    return\n",
    "\n",
    "  if not dependencies_installed:\n",
    "    print(\"\ud83c\udfed Instalando entrenador...\")\n",
    "    t0 = time.time()\n",
    "    install_trainer()\n",
    "    t1 = time.time()\n",
    "    dependencies_installed = True\n",
    "    print(f\"\u2705 Instalaci\u00f3n terminada en {int(t1 - t0)} segundos.\")\n",
    "  else:\n",
    "    print(\"\u2705 Dependencias ya instaladas.\")\n",
    "\n",
    "  if old_model_url != model_url or not model_file or not os.path.exists(model_file):\n",
    "    print(\"\ud83d\udd04 Obteniendo modelo...\")\n",
    "    if not download_model():\n",
    "      print(\"\ud83d\udca5 Error: el modelo que especific\u00f3 no es v\u00e1lido o est\u00e1 corrupto. Verifique que la URL sea accesible o que la ruta exista dentro de su espacio Lightning.\")\n",
    "      return\n",
    "    print()\n",
    "  else:\n",
    "    print(\"\ud83d\udd04 Modelo ya disponible.\")\n",
    "\n",
    "  if lr_scheduler_type:\n",
    "    create_config()\n",
    "    os.chdir(kohya_dir)\n",
    "    calculate_rex_steps()\n",
    "    os.chdir(root_dir)\n",
    "\n",
    "  create_config()\n",
    "\n",
    "  print(\"\u2b50 Iniciando Entrenador..\")\n",
    "\n",
    "  os.chdir(kohya_dir)\n",
    "  !{venv_python} {train_network} --config_file={config_file} --dataset_config={dataset_config_file}\n",
    "  os.chdir(root_dir)\n",
    "\n",
    "  if not get_ipython().__dict__.get('user_ns', {}).get('_exit_code', False):\n",
    "    display(Markdown(f\"### \u2705 \u00a1Hecho! Tus archivos se encuentran en `{output_folder}`\"))\n",
    "\n",
    "main()\n",
    "print(\"\ud83d\udd35 El cuaderno continuar\u00e1 en ejecuci\u00f3n. Det\u00e9n manualmente la sesi\u00f3n de Lightning cuando termines.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zci9GW_i831B"
   },
   "source": [
    "## Opcional para no perder creditos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#elimina los modelos base antes de cerrar lightning para que no te cobren mas creditos por almacenamiento.\n",
    "!rm -rf models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zci9GW_i831B"
   },
   "source": [
    "## Extras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ODpo0kcX3KRy"
   },
   "source": [
    "### \ud83d\udcda Varias carpetas en el mismo conjunto de datos\n",
    "A continuaci\u00f3n se muestra una plantilla que le permite definir varias carpetas en su conjunto de datos. Debe incluir la ubicaci\u00f3n de cada carpeta y puede establecer un n\u00famero diferente de repeticiones para cada una. Para agregar m\u00e1s carpetas, simplemente copie y pegue las secciones que comienzan con `[[datasets.subsets]]`.\n",
    "\n",
    "Al habilitar esto, se ignorar\u00e1 el n\u00famero de repeticiones establecidas en la celda principal y tambi\u00e9n se ignorar\u00e1 la carpeta principal establecida por el nombre del proyecto.\n",
    "\n",
    "Puede convertir uno de ellos en una carpeta de regularizaci\u00f3n agregando `is_reg = true`  \n",
    "Tambi\u00e9n puede establecer diferentes `keep_tokens`, `flip_aug`, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y037lagnJWmn"
   },
   "outputs": [],
   "source": [
    "custom_dataset = \"\"\"\n",
    "[[datasets]]\n",
    "\n",
    "[[datasets.subsets]]\n",
    "image_dir = \"/teamspace/studios/this_studio/lora_projects/example/dataset/good_images\"\n",
    "num_repeats = 3\n",
    "\n",
    "[[datasets.subsets]]\n",
    "image_dir = \"/teamspace/studios/this_studio/lora_projects/example/dataset/normal_images\"\n",
    "num_repeats = 1\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "23_YDYGhEnK0"
   },
   "outputs": [],
   "source": [
    "#@markdown ## Directorio base en Lightning.ai\n",
    "from pathlib import Path\n",
    "lightning_root = Path('/teamspace/studios/this_studio')\n",
    "print(f\"Trabajando en: {lightning_root}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W84Jxf-U2TIU"
   },
   "outputs": [],
   "source": [
    "custom_dataset = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "ImAtduziVp5h"
   },
   "outputs": [],
   "source": [
    "##Calculador de Repeticiones \u231b\ud83d\udcdd\n",
    "# Calcula el n\u00famero de repeticiones a usar para entrenar tu lora, Recuerda que en `SDXL y Pony` se usa un batch de `4`.\n",
    "# Si usas colab pro calcula tus repeticiones con `8` de batch size\n",
    "# Define las Variables\n",
    "# N\u00famero de im\u00e1genes\n",
    "num_images = 24 # @param{type:\"number\"}\n",
    "# N\u00famero de repeticiones\n",
    "num_repeats = 2 # @param{type:\"number\"}\n",
    "# N\u00famero de epocas\n",
    "num_epochs = 30 # @param{type:\"number\"}\n",
    "# Tama\u00f1o de lote\n",
    "batch_size = 8 # @param{type:\"number\"}\n",
    "\n",
    "# Calcula el resultado\n",
    "resultado = (num_images * num_repeats * num_epochs) / batch_size\n",
    "\n",
    "# Muestra el resultado\n",
    "print(\"\\33[96mEl total de repeticiones es:\\033[0m\", resultado)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "1RfwB4UB3M_w"
   },
   "outputs": [],
   "source": [
    "#@markdown ### \ud83d\udcc2 Descomprimir conjunto de datos\n",
    "#@markdown Sube un archivo `.zip` al almacenamiento de Lightning y descompr\u00edmelo en la carpeta deseada.\n",
    "zip = \"/teamspace/studios/this_studio/my_dataset.zip\" #@param {type:\"string\"}\n",
    "extract_to = \"/teamspace/studios/this_studio/lora_projects/example/dataset\" #@param {type:\"string\"}\n",
    "\n",
    "import os, zipfile\n",
    "\n",
    "os.makedirs(extract_to, exist_ok=True)\n",
    "with zipfile.ZipFile(zip, 'r') as f:\n",
    "  f.extractall(extract_to)\n",
    "\n",
    "print(\"\u2705 Archivo extra\u00eddo\")\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}